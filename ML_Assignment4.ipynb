{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### SVM & Naive Bayes"
      ],
      "metadata": {
        "id": "nV1tCmcerOYw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a Support Vector Machine (SVM), and how does it work?\n",
        "- A Support Vector Machine (SVM) is a supervised learning model that finds the decision boundary (hyperplane) that maximizes the margin—the distance to the nearest training points called support vectors—thereby improving generalization. For linearly separable data, it learns a maximal-margin hyperplane; with soft margins, it allows some misclassifications to balance margin size and errors. Using kernels, SVMs map inputs into higher-dimensional spaces to separate data that isn’t linearly separable in the original space. SVMs also extend to regression and can be adapted to multiclass via one-vs-rest or one-vs-one schemes.\n",
        "\n",
        "2. Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "- Hard margin SVM assumes perfectly linearly separable data and finds a hyperplane with the maximum margin while allowing zero misclassifications; it’s simple but highly sensitive to outliers and infeasible when classes overlap. Soft margin SVM relaxes this by introducing slack variables and a regularization parameter C, allowing some violations to balance margin width against classification errors; this makes it robust to noise and applicable to non-separable data. In practice, hard margin is rare; soft margin (tuning C) is the standard choice.\n",
        "\n",
        "3. What is the Kernel Trick in SVM? Give one example of a kernel and\n",
        "explain its use case.\n",
        "- The kernel trick lets an SVM learn nonlinear decision boundaries by implicitly mapping data into a higher-dimensional feature space without computing that mapping explicitly; instead, it uses a kernel function K(x, x′) that equals the dot product in the mapped space, keeping computation efficient. Example: the RBF (Gaussian) kernel K(x, x′) = exp(−γ‖x − x′‖²) creates flexible, localized decision regions; it’s well-suited when class boundaries are complex and not linearly separable, with γ controlling how tightly the influence of each training point decays with distance.\n",
        "\n",
        "4. What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "- A Naïve Bayes classifier is a simple probabilistic model that applies Bayes’ theorem to compute the posterior probability of each class given an input’s features and predicts the class with the highest posterior. It’s called “naïve” because it assumes conditional independence among features given the class—that each feature contributes to the class probability independently—which greatly simplifies computation and parameter estimation, often working well in high-dimensional tasks like text classification despite the assumption being unrealistic in many real datasets.\n",
        "\n",
        "5. Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants.\n",
        "When would you use each one?\n",
        "- Gaussian NB: assumes each continuous feature follows a class-conditional normal distribution; use for real-valued, approximately continuous data (e.g., sensor readings, measurements).\n",
        "\n",
        "- Multinomial NB: models counts/frequencies per feature with a multinomial likelihood; use for discrete count data like word counts or TF (or TF–IDF as an approximation) in document classification.\n",
        "\n",
        "- Bernoulli NB: models binary feature presence/absence; use when features are boolean indicators (e.g., whether a word appears at least once, on/off flags) or when sparsity and presence information matter more than counts."
      ],
      "metadata": {
        "id": "hiOj3rPerTBR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4nto8dVlrJF4",
        "outputId": "b3c04010-c5c7-49c4-efc1-9463f0a3f5f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0000\n",
            "Number of support vectors: 23 (per class: [3, 11, 9])\n",
            "First 5 support vectors:\n",
            " [[4.5 2.3 1.3 0.3]\n",
            " [4.8 3.4 1.9 0.2]\n",
            " [5.1 3.3 1.7 0.5]\n",
            " [6.8 2.8 4.8 1.4]\n",
            " [6.  2.9 4.5 1.5]]\n",
            "Support indices (first 20): [48, 63, 71, 2, 11, 20, 39, 53, 64, 67, 68, 82, 87, 118, 1, 5, 7, 55, 73, 75]\n"
          ]
        }
      ],
      "source": [
        "\"\"\" 6. Write a Python program to:\n",
        "● Load the Iris dataset\n",
        "● Train an SVM Classifier with a linear kernel\n",
        "● Print the model's accuracy and support vectors.\"\"\"\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "clf = SVC(kernel=\"linear\", C=1.0, random_state=42)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "support_vectors = clf.support_vectors_\n",
        "n_support = clf.n_support_\n",
        "support_idx = clf.support_\n",
        "\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "print(f\"Number of support vectors: {support_vectors.shape[0]} (per class: {n_support.tolist()})\")\n",
        "print(\"First 5 support vectors:\\n\", np.array2string(support_vectors[:5], precision=3))\n",
        "print(\"Support indices (first 20):\", support_idx[:20].tolist())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" 7. Write a Python program to:\n",
        "● Load the Breast Cancer dataset\n",
        "● Train a Gaussian Naïve Bayes model\n",
        "● Print its classification report including precision, recall, and F1-score.\"\"\"\n",
        "\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X, y = data.data, data.target\n",
        "target_names = data.target_names\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "y_pred = gnb.predict(X_test)\n",
        "print(classification_report(y_test, y_pred, target_names=target_names, digits=3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zXigNXgUuovM",
        "outputId": "5c51d351-a22c-4ae9-ae49-2e28678a5ddc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "   malignant      0.927     0.905     0.916        42\n",
            "      benign      0.945     0.958     0.952        72\n",
            "\n",
            "    accuracy                          0.939       114\n",
            "   macro avg      0.936     0.932     0.934       114\n",
            "weighted avg      0.938     0.939     0.938       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" 8. Write a Python program to:\n",
        "● Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best\n",
        "C and gamma.\n",
        "● Print the best hyperparameters and accuracy. \"\"\"\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "wine = load_wine()\n",
        "X, y = wine.data, wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "svc = SVC(kernel=\"rbf\")\n",
        "param_grid = {\n",
        "    \"C\": [0.1, 1, 3, 10, 30, 100],\n",
        "    \"gamma\": [\"scale\", \"auto\", 0.001, 0.003, 0.01, 0.03, 0.1]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator=svc,\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"accuracy\",\n",
        "    cv=5,\n",
        "    n_jobs=-1,\n",
        "    refit=True\n",
        ")\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best hyperparameters:\", grid.best_params_)\n",
        "print(f\"CV Best Score (mean accuracy): {grid.best_score_:.4f}\")\n",
        "print(f\"Test Accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EZnOg_zxu8Oy",
        "outputId": "fb1f846c-edb6-44ae-ec00-415e46d65be7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best hyperparameters: {'C': 3, 'gamma': 0.001}\n",
            "CV Best Score (mean accuracy): 0.7542\n",
            "Test Accuracy: 0.6944\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\" 9. Write a Python program to:\n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using\n",
        "sklearn.datasets.fetch_20newsgroups).\n",
        "● Print the model's ROC-AUC score for its predictions.\"\"\"\n",
        "\n",
        "# Python 3.x\n",
        "# Task: Train Naive Bayes on 20 Newsgroups text and report ROC-AUC\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import numpy as np\n",
        "\n",
        "categories = ['sci.space', 'rec.autos']\n",
        "data = fetch_20newsgroups(subset='all', categories=categories, remove=('headers', 'footers', 'quotes'))\n",
        "X_text, y = data.data, data.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_text, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "vect = TfidfVectorizer(\n",
        "    max_features=50000,\n",
        "    ngram_range=(1,2),\n",
        "    stop_words='english'\n",
        ")\n",
        "X_train_tfidf = vect.fit_transform(X_train)\n",
        "X_test_tfidf = vect.transform(X_test)\n",
        "\n",
        "clf = MultinomialNB(alpha=1.0)\n",
        "clf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "y_prob = clf.predict_proba(X_test_tfidf)[:, 1]\n",
        "auc = roc_auc_score(y_test, y_prob)\n",
        "print(f\"ROC-AUC: {auc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GTopTkz-wGuZ",
        "outputId": "ce61ed0d-e9a9-48e8-c803-95eff2eb6199"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC: 0.9914\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Imagine you’re working as a data scientist for a company that handles\n",
        "email communications.\n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may\n",
        "contain:\n",
        "● Text with diverse vocabulary\n",
        "● Potential class imbalance (far more legitimate emails than spam)\n",
        "● Some incomplete or missing data\n",
        "Explain the approach you would take to:\n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)\n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)\n",
        "● Address class imbalance\n",
        "● Evaluate the performance of your solution with suitable metrics\n",
        "And explain the business impact of your solution.\n",
        "\n",
        "-  Preprocess: join subject+body, lowercase, keep/normalize URLs/emails/numbers as tokens, fill missing text with empty string and add missing flags, vectorize with TF‑IDF using word (1–2) and character (3–5) n‑grams.\n",
        "-  Model: start with Multinomial Naïve Bayes as a fast baseline; prefer a linear SVM (LinearSVC) for stronger precision/recall on sparse text; calibrate SVM probabilities if thresholding risk.\n",
        "-  Imbalance: use stratified splits, class_weight=\"balanced\" (for SVM), tune decision threshold on validation to hit target precision/recall, and avoid SMOTE on raw text; consider modest undersampling of ham if needed.\n",
        "-  Tuning: cross-validate TF‑IDF params (ngram_range, max_features) and model params (NB alpha, SVM C), optimizing PR‑AUC/average precision.\n",
        "-  Evaluate: report precision, recall, F1, PR‑AUC; select an operating threshold and show confusion matrix; monitor performance over time and by segments (domain, language). Business impact: fewer spam/phishing emails reaching inboxes, reduced false blocks of legitimate mail, improved security/compliance, and lower manual review effort via calibrated, thresholdable risk scores."
      ],
      "metadata": {
        "id": "pzb1CFQtwkBB"
      }
    }
  ]
}