{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Logistic Regression"
      ],
      "metadata": {
        "id": "k4AeAVLFmV2D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Logistic Regression, and how does it differ from Linear\n",
        "Regression?\n",
        "- Logistic regression is a statistical method used for classification, especially for predicting the probability of a binary outcome (e.g., yes/no, 0/1). It uses the logistic (sigmoid) function to map predicted values to a range between 0 and 1. In contrast, linear regression predicts a continuous numeric value by fitting a straight-line relationship between input variables and the output. While linear regression uses least squares to minimize prediction error, logistic regression uses maximum likelihood estimation and focuses on probabilities rather than direct numeric predictions.\n",
        "\n",
        "2. Explain the role of the Sigmoid function in Logistic Regression.\n",
        "- The sigmoid (logistic) function maps the linear score z = w·x + b to a value in (0,1), turning it into a valid probability for the positive class and enabling a natural decision threshold (often 0.5). It also provides the link between log-odds and features: applying the inverse (logit) gives log(p/(1−p)) = w·x + b, which is central to maximum-likelihood training and interpretable odds ratios.\n",
        "\n",
        "3. What is Regularization in Logistic Regression and why is it needed?\n",
        "- Regularization in logistic regression adds a penalty to the loss function (typically L1 or L2) to discourage large coefficients, which reduces model complexity and helps prevent overfitting, improving generalization to unseen data. L2 regularization shrinks weights toward zero without making them exactly zero, stabilizing models under multicollinearity, while L1 can drive some weights to exactly zero, doubling as embedded feature selection; the penalty strength is controlled by a hyperparameter (λ), trading a bit of training accuracy for better test performance.\n",
        "\n",
        "4. What are some common evaluation metrics for classification models, and\n",
        "why are they important?\n",
        "- Common metrics for classification include accuracy, precision, recall, F1 score, ROC-AUC, PR-AUC, log loss, and the confusion matrix. They are important because each captures different aspects of performance; for example, precision and recall are key for imbalanced data, F1 balances them, ROC-AUC and PR-AUC assess discrimination across thresholds, and log loss measures the quality of probability predictions."
      ],
      "metadata": {
        "id": "QJKGDr_CmaMz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YF09sQqomJTO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01d97019-55e2-43bc-eab7-0b52d5b4dfbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9649\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "# 5. Write a Python program that loads a CSV file into a Pandas DataFrame, splits into train/test sets, trains a Logistic Regression model, and prints its accuracy. (Use Dataset from sklearn package)\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target, name=\"target\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "print(f\"Accuracy: {acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Write a Python program to train a Logistic Regression model using L2 regularization (Ridge) and print the model coefficients and accuracy. (Use Dataset from sklearn package)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target, name=\"target\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model = LogisticRegression(penalty='l2', solver='lbfgs', max_iter=1000)\n",
        "model.fit(X_train_scaled, y_train)\n",
        "\n",
        "coefficients = model.coef_.ravel()\n",
        "intercept = model.intercept_[0]\n",
        "\n",
        "y_pred = model.predict(X_test_scaled)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Intercept (bias):\", intercept)\n",
        "print(\"\\nCoefficients (feature -> weight):\")\n",
        "for fname, coef in zip(data.feature_names, coefficients):\n",
        "    print(f\"{fname}: {coef:.6f}\")\n",
        "\n",
        "print(f\"\\nAccuracy: {accuracy:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVRKz2ZtqCOY",
        "outputId": "6fa01503-ef7a-49ad-833a-c4914212a745"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Intercept (bias): 0.3022075735370281\n",
            "\n",
            "Coefficients (feature -> weight):\n",
            "mean radius: -0.511479\n",
            "mean texture: -0.552698\n",
            "mean perimeter: -0.476298\n",
            "mean area: -0.541059\n",
            "mean smoothness: -0.212479\n",
            "mean compactness: 0.648342\n",
            "mean concavity: -0.602103\n",
            "mean concave points: -0.704156\n",
            "mean symmetry: -0.167233\n",
            "mean fractal dimension: 0.199732\n",
            "radius error: -1.082965\n",
            "texture error: 0.248823\n",
            "perimeter error: -0.544333\n",
            "area error: -0.929104\n",
            "smoothness error: -0.160276\n",
            "compactness error: 0.647227\n",
            "concavity error: 0.160563\n",
            "concave points error: -0.443784\n",
            "symmetry error: 0.360492\n",
            "fractal dimension error: 0.437894\n",
            "worst radius: -0.947616\n",
            "worst texture: -1.255088\n",
            "worst perimeter: -0.763220\n",
            "worst area: -0.947756\n",
            "worst smoothness: -0.746625\n",
            "worst compactness: 0.055514\n",
            "worst concavity: -0.823151\n",
            "worst concave points: -0.953686\n",
            "worst symmetry: -0.939181\n",
            "worst fractal dimension: -0.187251\n",
            "\n",
            "Accuracy: 0.9825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr' and print the classification report. (Use Dataset from sklearn package)\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "iris = load_iris()\n",
        "X = pd.DataFrame(iris.data, columns=iris.feature_names)\n",
        "y = pd.Series(iris.target, name=\"target\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "clf = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=1000)\n",
        "clf.fit(X_train_scaled, y_train)\n",
        "\n",
        "y_pred = clf.predict(X_test_scaled)\n",
        "print(classification_report(y_test, y_pred, target_names=iris.target_names))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AJp6PvL1qbWW",
        "outputId": "d6903433-3e13-4746-a688-86e4062398c1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "      setosa       1.00      1.00      1.00        10\n",
            "  versicolor       0.89      0.80      0.84        10\n",
            "   virginica       0.82      0.90      0.86        10\n",
            "\n",
            "    accuracy                           0.90        30\n",
            "   macro avg       0.90      0.90      0.90        30\n",
            "weighted avg       0.90      0.90      0.90        30\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1256: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. Use OneVsRestClassifier(LogisticRegression(..)) instead. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Write a Python program to apply GridSearchCV to tune C and penalty hyperparameters for Logistic Regression and print the best parameters and validation accuracy. (Use Dataset from sklearn package)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target, name=\"target\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "pipe = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"logreg\", LogisticRegression(max_iter=1000))\n",
        "])\n",
        "\n",
        "param_grid = [\n",
        "    {\n",
        "        \"logreg__solver\": [\"liblinear\"],\n",
        "        \"logreg__penalty\": [\"l1\", \"l2\"],\n",
        "        \"logreg__C\": np.logspace(-3, 3, 7),\n",
        "    },\n",
        "    {\n",
        "        \"logreg__solver\": [\"lbfgs\", \"newton-cg\", \"sag\", \"saga\"],\n",
        "        \"logreg__penalty\": [\"l2\"],\n",
        "        \"logreg__C\": np.logspace(-3, 3, 7),\n",
        "    }\n",
        "]\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    estimator=pipe,\n",
        "    param_grid=param_grid,\n",
        "    scoring=\"accuracy\",\n",
        "    cv=5,\n",
        "    n_jobs=-1\n",
        ")\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "test_acc = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Best parameters:\", grid.best_params_)\n",
        "print(f\"Best CV accuracy: {grid.best_score_:.4f}\")\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pB5Veim4rPdI",
        "outputId": "3712b55b-5c93-46bf-e87d-120a82eadec9"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'logreg__C': np.float64(0.1), 'logreg__penalty': 'l2', 'logreg__solver': 'liblinear'}\n",
            "Best CV accuracy: 0.9802\n",
            "Test accuracy: 0.9825\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Write a Python program to standardize the features before training Logistic Regression and compare the model's accuracy with and without scaling. (Use Dataset from sklearn package)\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "data = load_breast_cancer()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = pd.Series(data.target, name=\"target\")\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "clf_no_scale = LogisticRegression(max_iter=1000)\n",
        "clf_no_scale.fit(X_train, y_train)\n",
        "y_pred_no_scale = clf_no_scale.predict(X_test)\n",
        "acc_no_scale = accuracy_score(y_test, y_pred_no_scale)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "clf_scaled = LogisticRegression(max_iter=1000)\n",
        "clf_scaled.fit(X_train_scaled, y_train)\n",
        "y_pred_scaled = clf_scaled.predict(X_test_scaled)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"Accuracy without scaling: {acc_no_scale:.4f}\")\n",
        "print(f\"Accuracy with scaling:    {acc_scaled:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eQX8FJ5yF8y",
        "outputId": "31854039-2572-46ee-c267-da9659cc9838"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.9649\n",
            "Accuracy with scaling:    0.9825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Imagine you are working at an e-commerce company that wants to\n",
        "predict which customers will respond to a marketing campaign. Given an imbalanced\n",
        "dataset (only 5% of customers respond), describe the approach you’d take to build a\n",
        "Logistic Regression model — including data handling, feature scaling, balancing\n",
        "classes, hyperparameter tuning, and evaluating the model for this real-world business\n",
        "use case.\n",
        "\n",
        "- For a 5% response rate, start with careful preprocessing: fix label leakage, impute missing values, one-hot encode categoricals, and standardize/normalize numeric features because Logistic Regression assumes features on comparable scales for stable coefficients and calibration. Handle imbalance primarily via cost-sensitive learning: set class_weight to “balanced” (≈ inverse frequency) or tune custom weights to emphasize responders without exploding false positives. Optionally compare data-level methods on the training fold only: SMOTE to oversample minority examples (fit SMOTE inside the CV pipeline to avoid leakage) versus modest undersampling; SMOTE often improves minority recall but can add noise if classes overlap, so validate carefully. Build a pipeline: scaler → (optional) SMOTE → LogisticRegression with L2 (or L1 for sparsity) and class weights; hyperparameter-tune C, penalty (L1/L2), solver, and class_weight via stratified cross-validation optimizing PR AUC or F2 (if recall matters more) rather than accuracy/ROC AUC alone, since PR AUC better reflects performance at low prevalence. Evaluate on a held-out test set with PR AUC, recall/precision, F1/F2, calibration (reliability curve/Brier), and business-costed metrics; choose a decision threshold by maximizing expected profit or meeting ops constraints (e.g., cap contact volume while targeting a minimum precision). Finally, check stability across time splits, inspect coefficient signs for sanity, and monitor drift and calibration post-deployment, retraining and retuning weights/thresholds as class balance or campaign economics change."
      ],
      "metadata": {
        "id": "7__3K9B6pElj"
      }
    }
  ]
}