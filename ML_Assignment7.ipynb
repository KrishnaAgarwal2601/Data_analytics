{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### KNN & PCA"
      ],
      "metadata": {
        "id": "vKm4x74wdVku"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is K-Nearest Neighbors (KNN) and how does it work in both\n",
        "classification and regression problems?\n",
        "- K-Nearest Neighbors (KNN) is a non-parametric, instance-based supervised learning method that predicts by looking at the K closest training samples to a query point using a distance metric (commonly Euclidean) and aggregating their targets. In classification, KNN assigns the class by majority (technically plurality) vote among the K neighbors, effectively using the neighbors’ mode as the predicted label. In regression, it predicts a continuous value by averaging (or taking the median of) the neighbors’ target values, making the output the neighbors’ mean/median. K controls the bias–variance trade-off: small K can be noisy but sensitive to local structure, while larger K smooths predictions and reduces sensitivity to outliers; the algorithm remains “lazy,” doing most computation at query time by storing the dataset and searching neighbors.\n",
        "\n",
        "2. What is the Curse of Dimensionality and how does it affect KNN\n",
        "performance?\n",
        "- The Curse of Dimensionality refers to phenomena that arise as feature count grows, where data becomes sparse, distances concentrate, and the amount of data needed grows exponentially, degrading learning and search efficiency. For KNN, this means distance metrics lose discriminative power (nearest and farthest neighbors become similarly distant), making neighbor selection noisy, increasing variance/overfitting, and demanding much more data and careful preprocessing (scaling, feature selection, dimensionality reduction) to maintain performance.\n",
        "\n",
        "3. What is Principal Component Analysis (PCA)? How is it different from\n",
        "feature selection?\n",
        "- PCA is a dimensionality reduction technique that linearly transforms correlated features into a smaller set of orthogonal principal components that capture the maximum variance, typically computed via eigen decomposition or SVD of the covariance matrix and used for compression, de-noising, and visualization. Unlike feature selection, which keeps a subset of original features, PCA performs feature extraction by creating new rotated features (linear combinations), so components may be less interpretable but often reduce multicollinearity and retain most information with fewer dimensions.\n",
        "\n",
        "4. What are eigenvalues and eigenvectors in PCA, and why are they\n",
        "important?\n",
        "- In PCA, eigenvectors are the orthogonal directions (principal axes) along which the data variance is maximized, and eigenvalues are the amounts of variance captured along each such direction, obtained by solving $$A v = \\lambda v$$ for the data covariance matrix A. The top eigenvectors (with the largest eigenvalues) define the principal components used to re-express data, so selecting them ranks components by explained variance and enables dimensionality reduction while preserving the most information.\n",
        "\n",
        "5. How do KNN and PCA complement each other when applied in a single\n",
        "pipeline?\n",
        "- PCA and KNN work well together because PCA creates a lower-dimensional set of uncorrelated components that preserve most variance, which reduces noise and distance “concentration” so KNN’s distance-based neighbor search becomes more discriminative and faster to compute. PCA also mitigates multicollinearity and scales features implicitly (after standardization), helping KNN avoid overfitting in high dimensions while often improving accuracy and inference latency; the typical pipeline is: standardize → PCA (retain components for, say, 95% variance) → KNN."
      ],
      "metadata": {
        "id": "AVOdgBGMdX50"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhDpdA_8c3oN",
        "outputId": "0751ba6b-bdf6-433f-c923-e057cd9f8593"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scikit-learn version: 1.6.1\n",
            "Accuracy without scaling: 0.7778\n",
            "Accuracy with scaling   : 0.9333\n",
            "\n",
            "Results:\n",
            "            Setting  Test Accuracy\n",
            "0  Without Scaling       0.777778\n",
            "1     With Scaling       0.933333\n"
          ]
        }
      ],
      "source": [
        "# 6. Train a KNN Classifier on the Wine dataset with and without feature scaling. Compare model accuracy in both cases.\n",
        "# KNN on Wine dataset: with vs without scaling\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "\n",
        "print(\"scikit-learn version:\", sklearn.__version__)\n",
        "\n",
        "data = load_wine(as_frame=True)\n",
        "X = data.data\n",
        "y = data.target\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "knn_no_scale = KNeighborsClassifier(n_neighbors=5, metric=\"minkowski\", p=2)\n",
        "knn_no_scale.fit(X_train, y_train)\n",
        "y_pred_no = knn_no_scale.predict(X_test)\n",
        "acc_no = accuracy_score(y_test, y_pred_no)\n",
        "\n",
        "pipe_scaled = Pipeline(steps=[\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"knn\", KNeighborsClassifier(n_neighbors=5, metric=\"minkowski\", p=2))\n",
        "])\n",
        "pipe_scaled.fit(X_train, y_train)\n",
        "y_pred_scaled = pipe_scaled.predict(X_test)\n",
        "acc_scaled = accuracy_score(y_test, y_pred_scaled)\n",
        "\n",
        "print(f\"Accuracy without scaling: {acc_no:.4f}\")\n",
        "print(f\"Accuracy with scaling   : {acc_scaled:.4f}\")\n",
        "\n",
        "results = pd.DataFrame({\n",
        "    \"Setting\": [\"Without Scaling\", \"With Scaling\"],\n",
        "    \"Test Accuracy\": [acc_no, acc_scaled]\n",
        "})\n",
        "print(\"\\nResults:\\n\", results)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. Train a PCA model on the Wine dataset and print the explained variance ratio of each principal component.\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "wine = load_wine(as_frame=True)\n",
        "X = wine.data\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_std = scaler.fit_transform(X)\n",
        "\n",
        "pca = PCA(n_components=None, random_state=42)\n",
        "pca.fit(X_std)\n",
        "\n",
        "evr = pca.explained_variance_ratio_\n",
        "cum_evr = np.cumsum(evr)\n",
        "\n",
        "print(\"Explained variance ratio per component:\")\n",
        "for i, (r, c) in enumerate(zip(evr, cum_evr), start=1):\n",
        "    print(f\"PC{i:02d}: {r:.4f}  |  Cumulative: {c:.4f}\")\n",
        "\n",
        "df = pd.DataFrame({\n",
        "    \"PC\": [f\"PC{i}\" for i in range(1, len(evr)+1)],\n",
        "    \"ExplainedVarianceRatio\": evr,\n",
        "    \"CumulativeEVR\": cum_evr\n",
        "})\n",
        "print(\"\\nTable:\\n\", df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3BFrijzOgHEl",
        "outputId": "118cf932-660e-4c99-af41-7f4e5fda8f75"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Explained variance ratio per component:\n",
            "PC01: 0.3620  |  Cumulative: 0.3620\n",
            "PC02: 0.1921  |  Cumulative: 0.5541\n",
            "PC03: 0.1112  |  Cumulative: 0.6653\n",
            "PC04: 0.0707  |  Cumulative: 0.7360\n",
            "PC05: 0.0656  |  Cumulative: 0.8016\n",
            "PC06: 0.0494  |  Cumulative: 0.8510\n",
            "PC07: 0.0424  |  Cumulative: 0.8934\n",
            "PC08: 0.0268  |  Cumulative: 0.9202\n",
            "PC09: 0.0222  |  Cumulative: 0.9424\n",
            "PC10: 0.0193  |  Cumulative: 0.9617\n",
            "PC11: 0.0174  |  Cumulative: 0.9791\n",
            "PC12: 0.0130  |  Cumulative: 0.9920\n",
            "PC13: 0.0080  |  Cumulative: 1.0000\n",
            "\n",
            "Table:\n",
            "       PC  ExplainedVarianceRatio  CumulativeEVR\n",
            "0    PC1                0.361988       0.361988\n",
            "1    PC2                0.192075       0.554063\n",
            "2    PC3                0.111236       0.665300\n",
            "3    PC4                0.070690       0.735990\n",
            "4    PC5                0.065633       0.801623\n",
            "5    PC6                0.049358       0.850981\n",
            "6    PC7                0.042387       0.893368\n",
            "7    PC8                0.026807       0.920175\n",
            "8    PC9                0.022222       0.942397\n",
            "9   PC10                0.019300       0.961697\n",
            "10  PC11                0.017368       0.979066\n",
            "11  PC12                0.012982       0.992048\n",
            "12  PC13                0.007952       1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. Train a KNN Classifier on the PCA-transformed dataset (retain top 2 components). Compare the accuracy with the original dataset.\n",
        "# KNN on original vs PCA(2) features - Wine dataset\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sklearn\n",
        "\n",
        "print(\"scikit-learn version:\", sklearn.__version__)\n",
        "\n",
        "wine = load_wine(as_frame=True)\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "pipe_base = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"knn\", KNeighborsClassifier(n_neighbors=5))\n",
        "])\n",
        "pipe_base.fit(X_train, y_train)\n",
        "y_pred_base = pipe_base.predict(X_test)\n",
        "acc_base = accuracy_score(y_test, y_pred_base)\n",
        "\n",
        "pipe_pca2 = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"pca\", PCA(n_components=2, random_state=42)),\n",
        "    (\"knn\", KNeighborsClassifier(n_neighbors=5))\n",
        "])\n",
        "pipe_pca2.fit(X_train, y_train)\n",
        "y_pred_pca2 = pipe_pca2.predict(X_test)\n",
        "acc_pca2 = accuracy_score(y_test, y_pred_pca2)\n",
        "\n",
        "print(f\"Accuracy on standardized original features: {acc_base:.4f}\")\n",
        "print(f\"Accuracy on PCA(2) features              : {acc_pca2:.4f}\")\n",
        "\n",
        "results = pd.DataFrame({\n",
        "    \"Setting\": [\"Std + KNN (original 13)\", \"Std + PCA(2) + KNN\"],\n",
        "    \"Test Accuracy\": [acc_base, acc_pca2]\n",
        "})\n",
        "print(\"\\nResults:\\n\", results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kgxyXQl4goYC",
        "outputId": "b9399e14-bda1-49ae-f172-4aa13d5fd219"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scikit-learn version: 1.6.1\n",
            "Accuracy on standardized original features: 0.9333\n",
            "Accuracy on PCA(2) features              : 0.9333\n",
            "\n",
            "Results:\n",
            "                    Setting  Test Accuracy\n",
            "0  Std + KNN (original 13)       0.933333\n",
            "1       Std + PCA(2) + KNN       0.933333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Train a KNN Classifier with different distance metrics (euclidean, manhattan) on the scaled Wine dataset and compare the results.\n",
        "# KNN with Euclidean vs Manhattan distance on scaled Wine dataset\n",
        "\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "\n",
        "print(\"scikit-learn version:\", sklearn.__version__)\n",
        "\n",
        "wine = load_wine(as_frame=True)\n",
        "X = wine.data\n",
        "y = wine.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.25, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "pipe_euclid = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"knn\", KNeighborsClassifier(n_neighbors=5, metric=\"minkowski\", p=2))\n",
        "])\n",
        "\n",
        "pipe_manhat = Pipeline([\n",
        "    (\"scaler\", StandardScaler()),\n",
        "    (\"knn\", KNeighborsClassifier(n_neighbors=5, metric=\"minkowski\", p=1))\n",
        "])\n",
        "\n",
        "pipe_euclid.fit(X_train, y_train)\n",
        "y_pred_e = pipe_euclid.predict(X_test)\n",
        "acc_e = accuracy_score(y_test, y_pred_e)\n",
        "\n",
        "pipe_manhat.fit(X_train, y_train)\n",
        "y_pred_m = pipe_manhat.predict(X_test)\n",
        "acc_m = accuracy_score(y_test, y_pred_m)\n",
        "\n",
        "print(f\"Accuracy (Euclidean, p=2): {acc_e:.4f}\")\n",
        "print(f\"Accuracy (Manhattan, p=1): {acc_m:.4f}\")\n",
        "\n",
        "results = pd.DataFrame({\n",
        "    \"Distance\": [\"Euclidean (p=2)\", \"Manhattan (p=1)\"],\n",
        "    \"Test Accuracy\": [acc_e, acc_m]\n",
        "})\n",
        "print(\"\\nResults:\\n\", results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_gpT3Vrtg4Re",
        "outputId": "8240c039-c650-4a22-f5dc-08bb4639d1bf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scikit-learn version: 1.6.1\n",
            "Accuracy (Euclidean, p=2): 0.9333\n",
            "Accuracy (Manhattan, p=1): 0.9778\n",
            "\n",
            "Results:\n",
            "           Distance  Test Accuracy\n",
            "0  Euclidean (p=2)       0.933333\n",
            "1  Manhattan (p=1)       0.977778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. You are working with a high-dimensional gene expression dataset to\n",
        "classify patients with different types of cancer.\n",
        "Due to the large number of features and a small number of samples, traditional models\n",
        "overfit.\n",
        "Explain how you would:\n",
        "● Use PCA to reduce dimensionality\n",
        "● Decide how many components to keep\n",
        "● Use KNN for classification post-dimensionality reduction\n",
        "● Evaluate the model\n",
        "● Justify this pipeline to your stakeholders as a robust solution for real-world\n",
        "biomedical data\n",
        "- Use PCA to reduce dimensionality: Standardize gene expression (e.g., log-transform counts, normalize, then z-score) and run PCA to project tens of thousands of correlated genes into a compact set of orthogonal components that capture the dominant biological variation while filtering noise and batch structure common in high-throughput assays.  \n",
        "- Decide how many components to keep: Inspect the explained variance via a scree plot and choose the “elbow,” or retain components until a cumulative variance threshold (typically 90–95%) is reached; validate this choice with nested cross-validation to ensure downstream performance stabilizes and is not sensitive to the exact component count.  \n",
        "- Use KNN after reduction: Train KNN on the retained PCs (with distances computed in the reduced space) since neighbor search becomes more discriminative and less prone to the curse of dimensionality; tune K (and distance metric) via cross-validation to balance bias–variance in the low-dimensional manifold.  \n",
        "- Evaluate the model: Use stratified nested cross-validation to tune hyperparameters and estimate generalization, reporting accuracy alongside recall, precision, F1 (per class) and macro-averages due to class imbalance; include calibration checks, confusion matrices, and, if applicable, external validation or repeated splits to confirm stability, plus sensitivity analyses to the number of PCs.  \n",
        "- Justify the pipeline: PCA+KNN directly addresses p≫n overfitting by compressing correlated gene signals into data-driven “metagenes” that preserve signal while reducing variance and computational load; it is transparent, reproducible, and robust for biomedical data where interpretability and validation are critical, and it integrates standard QC steps (normalization, batch assessment), principled model selection, and rigorous validation suitable for clinical research workflows."
      ],
      "metadata": {
        "id": "5wkxaWbLhllw"
      }
    }
  ]
}